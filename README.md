<!-- omit in toc -->
# NLP-with-classification-and-vector-spaces

A repository is dedicated to Natural Language Processing with classification and vector spaces (a course from Coursera @DeepLearning.AI).    

In this repository, I will be documenting down what I've learned from the course and projects. Projects completed can be found in sub-directories. See [Contents](#contents) below for more information. 

*Credit:*  @DeepLearning.AI


<!-- omit in toc -->
## Contents 
------


- [Sentiment Analysis with Logistic Regression](#sentiment-analysis-with-logistic-regression)
  - [Goal(s)](#goals)
  - [Files](./logistic-regression/)
- [Sentiment Analysis with Naïve Bayes](#sentiment-analysis-with-naive-bayes)
    - [Goal(s)](#goals-1)
    - [Files](./naive-bayes/)
- [Vector Space Model](#overloading-the-stream-insertion-operator)
    - [Goal(s)](#goals-2)
    - [Files](./vector-space-model/)
- [Machine Translation and Document Search](#machine-translation-and-document-search)
  - [Goal(s)](#goals-3)
  - [Files](./machine-translation-and-document-search)

Sentiment Analysis with Logistic Regression
----------------------------
### Goals: 
To be able to extract features from text into numerical vectors, then build a binary classifier for tweets using a logistic regression!     

Concepts:     
- Sentiment analysis
- Logistic regression
- Data pre-processing
- Calculating word frequencies
- Feature extraction
- Vocabulary creation
- Supervised learning       

Sentiment Analysis with Naïve Bayes
----------------------------
### Goals:
Learn the theory behind Bayes' rule for conditional probabilities, then apply it toward building a Naive Bayes tweet classifier!      

Concepts:     
- Error analysis
- Naive Bayes inference
- Log likelihood
- Laplacian smoothing
- conditional probabilities
- Bayes rule
- Sentiment analysis
- Vocabulary creation
- Supervised learning

Vector Space Model
------------------
### Goals: 
Vector space models capture semantic meaning and relationships between words. The goal is to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.

Concepts: 
- Covariance matrices
- Dimensionality reduction
- Principal component analysis
- Cosine similarity
- Euclidean distance
- Co-occurrence matrices
- Vector representations
- Vector space models     

Machine Translation and Document Search 
---------------------------------------
### Goals:     
In order to perform machine translation and document search, we will learn to transform word vectors and assign them to subsets using locality sensitive hashing.     

Concepts:     
- Gradient descent
- Approximate nearest neighbors
- Locality sensitive hashing
- Hash functions
- Hash tables
- K nearest neighbors
- Document search
- Machine translation
- Frobenius norm



